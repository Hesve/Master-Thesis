rnn_unit <- function(W,U,b, h_t_minus_one, x_t){
at <- b + W %*% as.vector(h_t_minus_one) + U %*% t(X)
# b + W%*%h_t_minus_one + U %*% x_t
#4 x 1 + 4x4 * 3*1  + 4x5 * 1x 5
return(at)
}
a_t <- rnn_unit(h_t_minus_one =h_t_minus_one, x_t=X, W = rnn_example$W, U = rnn_example$U, b = rnn_example$b)
a_t
activation <- function(x){
return((exp(x) - exp(-x)) / (exp(x) + exp(-x)))
}
h_t <- activation(a_t)
h_t
output_rnn<- function(h_t, V, c){
return(c + V %*% h_t)
}
softmax <- function(o){
return(exp(o) / sum(exp(o)))
}
softmax(output_rnn(h_t, rnn_example$V, rnn_example$c) )
rnn_layer(X, W = rnn_example$W, V = rnn_example$V, U = rnn_example$U, rnn_example$b, rnn_example$c)
rnn_layer <- function(X, W, V, U, b, c, hidden_dim = 4){
h_t_minus_one <- matrix(0, nrow = hidden_dim, ncol = 1)
a_t <- rnn_unit(h_t_minus_one, x_t=X, W =W, U=U, b = b)
ht <- activation(a_t)
o <- output_rnn(ht, V, c)
y_hat <- activation(o)
return(list(ht=ht, y_hat=y_hat))
}
rnn_layer(X, W = rnn_example$W, V = rnn_example$V, U = rnn_example$U, rnn_example$b, rnn_example$c)
rnn_layer <- function(X, W, V, U, b, c, hidden_dim = 4){
h_t_minus_one <- matrix(0, nrow = hidden_dim, ncol = 1)
a_t <- rnn_unit(h_t_minus_one, x_t=X, W =W, U=U, b = b)
ht <- activation(a_t)
o <- output_rnn(ht, V, c)
print(o)
y_hat <- activation(o)
return(list(ht=ht, y_hat=y_hat))
}
rnn_layer(X, W = rnn_example$W, V = rnn_example$V, U = rnn_example$U, rnn_example$b, rnn_example$c)
rnn_layer(X, W = rnn_example$W, V = rnn_example$V, U = rnn_example$U, rnn_example$b,c=  rnn_example$c)
softmax(output_rnn(h_t, rnn_example$V, rnn_example$c) )
rnn_layer <- function(X, W, V, U, b, c, hidden_dim = 4){
h_t_minus_one <- matrix(0, nrow = hidden_dim, ncol = 1)
a_t <- rnn_unit(h_t_minus_one, x_t=X, W =W, U=U, b = b)
ht <- activation(a_t)
o <- output_rnn(ht, V, c)
cat("O is", o, "\n")
y_hat <- activation(o)
return(list(ht=ht, y_hat=y_hat))
}
rnn_layer(X, W = rnn_example$W, V = rnn_example$V, U = rnn_example$U, rnn_example$b,c=  rnn_example$c)
output_rnn(h_t, rnn_example$V, rnn_example$c)
softmax(output_rnn(h_t, rnn_example$V, rnn_example$c) )
rnn_layer <- function(X, W, V, U, b, c, hidden_dim = 4){
h_t_minus_one <- matrix(0, nrow = hidden_dim, ncol = 1)
a_t <- rnn_unit(h_t_minus_one, x_t=X, W =W, U=U, b = b)
ht <- activation(a_t)
o <- output_rnn(ht, V, c)
cat("O is", o, "\n")
y_hat <- softmax(o)
return(list(ht=ht, y_hat=y_hat))
}
rnn_layer(X, W = rnn_example$W, V = rnn_example$V, U = rnn_example$U, rnn_example$b,c=  rnn_example$c)
library(xtable)
library(FE)
data <- index_d
#log returns
lret = apply(log(data),2,diff)
lret_squared <- lret**2
lret_squared
summary(lret_squared)
summary <- function(data){
}
library(fGarch)
# Generate log returns
lret <- apply(log(index_d),2,diff)
t1 <- summary(lret)
xtable(t1)
# Generate Squared log returns
lret2 <- lret^2
t2 <- summary(lret2)
xtable(t2)
# FOR SINGAPORE
mod2 <- garchFit(formula = ~arma(1, 0) + garch(3, 0), data = as.numeric(lret[,6], trace = TRUE))
summary(mod2)
summary(mod2)
# FOR SINGAPORE
mod2 <- garchFit(formula = ~arma(1, 0) + garch(3, 0), data = as.numeric(lret[,6], trace = TRUE))
summary(mod2)
summary(mod2)
mod2@call
mod2@residuals
residuals <- mod2@residuals
squared_residuals <- mod2@residuals^2
residuals <-
# Create lagged squared residuals
lagged_squared_residuals <- lag(residuals^2, k = 1:5)
lag(residuals5)
lag(residuals^2, 5)
library(tseries)
library(quantmod)
# Create lagged squared residuals
lagged_squared_residuals <- Lag(residuals^2, k = 1:5, na.pad = TRUE)
# Create lagged squared residuals
lagged_squared_residuals <- Lag(residuals^2, k = 1:5)
lagged_squared_residuals
# Combine lagged squared residuals and squared residuals into a data frame
aux_reg_data <- data.frame(residuals_sq = residuals^2, lagged_squared_residuals)
# Fit auxiliary regression
aux_reg_model <- lm(residuals_sq ~ ., data = aux_reg_data)
summary(aux_reg_model)
summary(aux_reg_model)
# Fit auxiliary regression
aux_reg_model <- lm(residuals_sq ~ ., data = aux_reg_data)
summary(aux_reg_model)
summary(aux_reg_model)
\end{document}
\end{document}
within_scatter <- compute_within_point_scatter(X, C)
within_point_scatter <- function(X, C) {
# Initialize within-point scatter
W <- 0
browser()
# Iterate through data points
for (i in 1:nrow(X)) {
# Get cluster label for data point i
k <- C[i]
# Get mean of cluster k
mean_k <- colMeans(X[C == k, ])
# Calculate distance to cluster mean
dist_to_mean <- sum((X[i, ] - mean_k)^2)
# Update within-point scatter
W <- W + dist_to_mean
}
# Return within-point scatter
return(W / 2)
}
within_scatter <- compute_within_point_scatter(X, C)
within_scatter <- within_point_scatter(X, C)
within_scatter <- within_point_scatter(X, C)
#Libraries
library(tidyverse)
library(xtable)
library(tensorflow)
library(keras)
data("iris")
data("faithful")
library(uuml)
data("mixture_data")
#' @param X n X p matrix
#' @param c p x 1 vector of cluster assignments
#' n is the number of rows, p is the number of cols
compute_cluster_means <- function(X, C) {
means <- t(sapply(sort(unique(C)), FUN = function(i){
index <- which(C == i)
colMeans(X[index,])
}))
colnames(means) <- colnames(X)
return(means)
}
set.seed(4711)
X <- as.matrix(faithful)
C <- sample(1: 3, nrow(X), replace = TRUE)
m <- compute_cluster_means(X,C)
m
#function to computer the squared distance given cluster means for one obs
#assuming the number of columns match .
cluster_dist <- function(x_i, cluster_means){
res <- t(apply(cluster_means, MARGIN=1, FUN= function(row){x_i - row}))
sums <- rowSums(res^2)
min_cluster <- which.min(sums)
return(min_cluster)
}
#' @param X n×p (design) matrix X
#' @param m  K×p matrix m with one cluster mean per row
#' @return  n × 1 vector of cluster assignments
compute_cluster_encoding <- function(X, means) {
new_clusters <- sapply(1:nrow(X), FUN = function(i){ cluster_dist(X[i,], means)})
return(new_clusters)
}
C <- compute_cluster_encoding(X, m)
C[1: 10]
k_means <- function(X,C){
clusters <- sample(1:C, nrow(X), replace=TRUE) #initialzie cluster assignments
while(TRUE){
c_means <- compute_cluster_means(X, clusters)
new_clusters <- compute_cluster_encoding(X, c_means)
if(identical(new_clusters, clusters)){
break
}
clusters <- new_clusters
}
return(clusters)
}
set.seed(4711)
k_means(X,3)
set.seed(4711)
X <- as.matrix(faithful)
C <- sample(1:3, nrow(X), replace = TRUE)
within_scatter <- within_point_scatter(X, C)
k
mean_k
dist_to_mean
within_point_scatter <- function(X, C) {
# Initialize within-point scatter
W <- 0
# Iterate through data points
for (i in 1:nrow(X)) {
# Get cluster label for data point i
k <- C[i]
# Get mean of cluster k
mean_k <- colMeans(X[C == k, ])
# Calculate distance to cluster mean
dist_to_mean <- sum((X[i, ] - mean_k)^2)
# Update within-point scatter
W <- W + dist_to_mean
}
# Return within-point scatter
return(W / 2)
}
within_scatter <- within_point_scatter(X, C)
print(within_scatter)
print(within_scatter)
k_means_W <- function(X,C){
browser()
c_means <- compute_cluster_means(X,C)
res <- sapply(1:nrow(c_means), FUN=function(i){
browser()
current_cluster_m <- c_means[i, ]
index <- which(C == i)
x_subset <- X[index,]
cluster_distances <- apply(x_subset, 1, function(row) sum((row - current_cluster_m)^2))
sum(cluster_distances)
})
return(res)
}
test <- k_means_W(X,C)
sum(k_means_W(X,C))
k_means_W <- function(X,C){
c_means <- compute_cluster_means(X,C)
res <- sapply(1:nrow(c_means), FUN=function(i){
current_cluster_m <- c_means[i, ]
index <- which(C == i)
x_subset <- X[index,]
cluster_distances <- apply(x_subset, 1, function(row) sum((row - current_cluster_m)^2))
sum(cluster_distances)
})
return(res)
}
test <- k_means_W(X,C)
sum(k_means_W(X,C))
sum(k_means_W(X,C))*3
sum(k_means_W(X,C))*3*3
tally(C)
table(C)
test <- k_means_W(X,C)
test * table(C)
Sum(test * table(C))
sum(test * table(C))
C
k_means_W <- function(X,C){
c_means <- compute_cluster_means(X,C)
within_cluster_sums <- res <- sapply(1:nrow(c_means), FUN=function(i){
current_cluster_m <- c_means[i, ]
index <- which(C == i)
x_subset <- X[index,]
cluster_distances <- apply(x_subset, 1, function(row) sum((row - current_cluster_m)^2))
sum(cluster_distances)
})
aggregated_res <- within_cluster_sums * table(C)
return(aggregated_res)
}
k_means_W()
k_means_W(X,C)
k_means_W <- function(X,C){
c_means <- compute_cluster_means(X,C)
within_cluster_sums <- res <- sapply(1:nrow(c_means), FUN=function(i){
current_cluster_m <- c_means[i, ]
index <- which(C == i)
x_subset <- X[index,]
cluster_distances <- apply(x_subset, 1, function(row) sum((row - current_cluster_m)^2))
sum(cluster_distances)
})
aggregated_res <- sum(within_cluster_sums * table(C))
return(aggregated_res)
}
k_means_W(X,C)
k_means_W <- function(X,C){
c_means <- compute_cluster_means(X,C)
within_cluster_sums<- sapply(1:nrow(c_means), FUN=function(i){
current_cluster_m <- c_means[i, ]
index <- which(C == i)
x_subset <- X[index,]
cluster_distances <- apply(x_subset, 1, function(row) sum((row - current_cluster_m)^2))
sum(cluster_distances)
})
aggregated_res <- sum(within_cluster_sums * table(C))
return(aggregated_res)
}
k_means_W(X,C)
kmeans(X, C)
kmeans(X, c)
k_means(X,C)
set.seed(4711)
k_means(X,3)
k_means(X,3)
k_means(X,3)
set.seed(4711)
k_means(X,3)
set.seed(4711)
k_means(X,3)
set.seed(4)
k_means(X,3)
set.seed(4711)
k_means(X,3)
set.seed(4)
k_means(X,3)
test <- k_means(X,3)
test
k_means_W(X,test)
k_means_wrapper(seed=4711, X, 3)
#wrapper functio to perform the k_means and k_means_W
k_means_wrapper <- function(seed, X, k ){
set.seed(seed)
result <- k_means(X,k)
wps <- k_means_W(X, result)
list <- list("Clusters" = result, "WPS" = wps)
return(list)
}
k_means_wrapper(seed=4711, X, 3)
rep(2,5)
k=c(rep(2,5), rep(3,5)))
mapply(FUN= k_means_wrapper(seed=c(1,2,3,4,5,6,7,8,9,10), X=list(X),
k=c(rep(2,5), rep(3,5))))
k_means_wrapper(4711, X, 3)
mapply(FUN= k_means_wrapper, seed=c(1,2,3,4,5,6,7,8,9,10), X=list(X),
k=c(rep(2,5), rep(3,5)))
multi_res <- mapply(FUN= k_means_wrapper, seed=c(1,2,3,4,5,6,7,8,9,10), X=list(X),
k=c(rep(2,5), rep(3,5)))
multi_res
View(multi_res)
multi_res <- mapply(FUN= k_means_wrapper, seed=c(1,2,3,4,5,6,7,8,9,10), X=list(X),
k=c(rep(2,5), rep(3,5)), SIMPLIFY = FALSE)
lapply(multi_res, FUN = function(list), list$WPS)
lapply(multi_res, FUN = function(list) {list$WPS})
do.call(rbind, multi_res)
do.call(rbind, multi_res)$WPS
do.call(rbind, multi_res)[,2]
which.min(do.call(rbind, multi_res)[,2])
which.max(do.call(rbind, multi_res)[,2])
lapply(multi_res, FUN = function(list) {list$WPS})
test \/ do.call(rbind, multi_res)
test <-  do.call(rbind, multi_res)
test$WPS
test[,2]
as.matrix(do.call(rbind, multi_res)[,2])
which.max(ds.matrix(do.call(rbind, multi_res)[,2]))
which.max(as.matrix(do.call(rbind, multi_res)[,2]))
merged_res <- do.call(rbind, multi_res)
merged_res
which.min(merged_res[,2])
which.max(merged_res[,2])
k_means_W(X,C)
mnist <- dataset_mnist()
#Libraries
library(tidyverse)
library(xtable)
library(tensorflow)
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x/255
x_test<- mnist$test$x/255
y_train <- to_categorical(mnist$train$y, num_classes = 10)
y_test <- to_categorical(mnist$test$y, num_classes = 10)
tensorflow::tf$compat$v1$disable_eager_execution()
if (tensorflow::tf$executing_eagerly())
tensorflow::tf$compat$v1$disable_eager_execution()
K <- keras::backend()
K
batch_size <- 100L
original_dim <- 784L
latent_dim <- 2L
intermediate_dim <- 256L
epochs <- 50L
epsilon_std <- 1.0
x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)
sampling <- function(arg){
z_mean <- arg[, 1:(latent_dim)]
z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
epsilon <- k_random_normal(
shape = c(k_shape(z_mean)[[1]]),
mean=0.,
stddev=epsilon_std
)
z_mean + k_exp(z_log_var/2)*epsilon
}
# note that "output_shape" isn't necessary with the TensorFlow backend
z <- layer_concatenate(list(z_mean, z_log_var)) %>%
layer_lambda(sampling)
# we instantiate these layers separately so as to reuse them later
decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)
# end-to-end autoencoder
vae <- keras_model(x, x_decoded_mean)
# encoder, from inputs to latent space
encoder <- keras_model(x, z_mean)
# generator, from latent space to reconstructed inputs
decoder_input <- layer_input(shape = latent_dim)
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input, x_decoded_mean_2)
vae_loss <- function(x, x_decoded_mean){
xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)
kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
xent_loss + kl_loss
}
vae %>% compile(optimizer = "rmsprop", loss = vae_loss)
mnist <- dataset_mnist()
x_train <- mnist$train$x/255
x_test <- mnist$test$x/255
x_train <- array_reshape(x_train, c(nrow(x_train), 784), order = "F")
x_test <- array_reshape(x_test, c(nrow(x_test), 784), order = "F")
vae %>% fit(
x_train, x_train,
shuffle = TRUE,
epochs = epochs,
batch_size = batch_size,
validation_data = list(x_test, x_test)
)
# Plot the image
plot(predict(generator, matrix(c(0,0), ncol = latent_dim)), col = gray.colors(256), main = "Latent State for Digit 2")
library(gptstudio)
remove.packages("gptstudio")
install.packages("gptstudio")
install.packages("gptstudio")
chattr:::chattr_app()
chattr_use("gpt35")
library(chattr)
chattr_use("gpt35")
chattr_test()
chattr_test()
chattr_app()
## importing data and processing
df <- read.csv("G:/My Drive/GitHub/introMLproject/data/filedir_df.csv")
# removing entries without an associated files
df <- df[!is.na(df$file_dir),]
# restricting analysis to bird calls
df <- df[df$TYPE = "song",]
# restricting analysis to bird calls
df <- df[df$TYPE = "song",]
# converting char time stamp to seconds
get_seconds <- function(charcode){
time <- strsplit(charcode, ":")[[1]]
seconds <- as.numeric(time[1])*60+as.numeric(time[2])
return(seconds)
}
df$recording_length_seconds <- sapply(df$recording_length, get_seconds)
# only look at clips up to 2 minutes
df <- df[df$recording_length_seconds <= 120,]
# only look at 5 most common genus
top_5_genus <- names(sort(table(factor(df$genus)), decreasing = T)[1:5])
df <- df[df$genus %in% top_5_genus, ]
df$TYPE=="song"
setwd("C:/Users/henry/OneDrive/Year_2/Master Thesis/Main/Code")
FED_data <- read.csv("./data/FEDFUNDS.csv")
library(haven)
library(readxl)
us_inflation <- read_excel("C:/Users/henry/OneDrive/Year_2/Master Thesis/Main/Code/data/us_inflation.xlsx",    skip = 10) # skip the first 10 rows as its just information
riksbanken_data <- read.csv("./data/riksbanken_monthly.csv")
swe_inflation <- read.csv("./data/sweden_inflation.csv")
FED_data <- read.csv("./data/FEDFUNDS.csv")
library(haven)
library(readxl)
us_inflation <- read_excel("C:/Users/henry/OneDrive/Year_2/Master Thesis/Main/Code/data/us_inflation.xlsx",    skip = 10) # skip the first 10 rows as its just information
riksbanken_data <- read.csv("./data/riksbanken_monthly.csv")
swe_inflation <- read.csv("./data/sweden_inflation.csv")
setwd("C:/Users/henry/OneDrive/Year_2/Master Thesis/Main/Code")
setwd("C:/Users/henry/OneDrive/Year_2/Master Thesis/Main/Code")
FED_data <- read.csv("./data/FEDFUNDS.csv")
library(haven)
library(readxl)
us_inflation <- read_excel("C:/Users/henry/OneDrive/Year_2/Master Thesis/Main/Code/data/us_inflation.xlsx",    skip = 10) # skip the first 10 rows as its just information
riksbanken_data <- read.csv("./data/riksbanken_monthly.csv")
swe_inflation <- read.csv("./data/sweden_inflation.csv")
swe_inflation <- read.csv("./data/swedish_CPI.csv")
riksbanken_data <- read.csv("./data/riksbanken_monthly.csv")
swe_inflation <- read_csv("data/swedish_CPI.csv",
col_names = FALSE, skip = 3)
library(readr)
library(haven)
library(readxl)
library(readr)
us_inflation <- read_excel("C:/Users/henry/OneDrive/Year_2/Master Thesis/Main/Code/data/us_inflation.xlsx",    skip = 10) # skip the first 10 rows as its just information
riksbanken_data <- read.csv("./data/riksbanken_monthly.csv")
swe_inflation <- read_csv("data/swedish_CPI.csv",
col_names = FALSE, skip = 3)
View(swe_inflation)
riksbanken_data <- read_delim("data/riksbanken_monthly.csv",
delim = ";", escape_double = FALSE, trim_ws = TRUE)
View(riksbanken_data)
View(FED_data)
View(riksbanken_data)
View(swe_inflation)
View(swe_inflation)
View(riksbanken_data)
View(FED_data)
